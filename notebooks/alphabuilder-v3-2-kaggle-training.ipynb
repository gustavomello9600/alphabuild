{
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.13"
        },
        "accelerator": "GPU",
        "kaggle": {
            "accelerator": "nvidiaTeslaT4",
            "dataSources": [
                {
                    "sourceId": 14005693,
                    "sourceType": "datasetVersion",
                    "datasetId": 8923815
                }
            ],
            "dockerImageVersionId": 31193,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": true
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "AlphaBuilder v3.2 - Kaggle Training Script (NEW REWARD)\n========================================================\n\nThis script is designed to run on Kaggle with GPU T4 x2.\nUpload this as a Kaggle notebook and enable GPU accelerator.\n\n**UPDATED**: Uses new additive reward formula for value head targets:\n- `compliance_score = 0.80 - 0.16 * (log10(C) - 1)`\n- `volume_bonus = (0.10 - V) * 2.0`  (always inversely proportional)\n- `reward = compliance_score + volume_bonus`\n\nExpected runtime: ~5 hours for 30 epochs (SimpleBackbone)\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Environment Setup\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\nimport subprocess\nimport sys\nimport os\n\nprint(\"=\"*60)\nprint(\"üöÄ AlphaBuilder v3.2 - Kaggle Training (NEW REWARD)\")\nprint(\"=\"*60)\n\n# Check GPU\nimport torch\nprint(f\"\\nüìä Hardware Detection:\")\nprint(f\"   CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU count: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n        props = torch.cuda.get_device_properties(i)\n        print(f\"       Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Clone Repository and Install Dependencies\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\nREPO_URL = \"https://github.com/gustavomello9600/alphabuild.git\"\nREPO_DIR = \"/kaggle/working/alphabuild\"\n\nif not os.path.exists(REPO_DIR):\n    print(f\"\\nüì• Cloning repository...\")\n    subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\nelse:\n    print(f\"\\nüì• Updating repository...\")\n    subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\"], check=True)\n\n# Add to path\nsys.path.insert(0, REPO_DIR)\nos.chdir(REPO_DIR)\n\nprint(f\"   Working directory: {os.getcwd()}\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Download Training Data\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\n# Option A: From Kaggle Dataset (recommended)\n# Add the dataset \"gustavomello9600/alphabuilder-warmup-data\" to your notebook\n\nDATA_PATH = \"/kaggle/input/alphabuilder-warmup-data/warmup_data_kaggle.db\"\n\n# Option B: From Google Drive (backup)\nif not os.path.exists(DATA_PATH):\n    print(\"\\n‚ö†Ô∏è Dataset not found in Kaggle input.\")\n    print(\"   Please add the dataset: gustavomello9600/alphabuilder-warmup-data\")\n    print(\"   Or upload warmup_data.db manually.\")\n    \n    # Try local path as fallback\n    DATA_PATH = \"/kaggle/working/alphabuild/data/warm_up_data/warmup_data.db\"\n    if not os.path.exists(DATA_PATH):\n        raise FileNotFoundError(f\"Training data not found at {DATA_PATH}\")\n\nprint(f\"\\nüìÇ Training data: {DATA_PATH}\")\nprint(f\"   Size: {os.path.getsize(DATA_PATH) / 1024**2:.1f} MB\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## NEW: Define Reward Functions\n\nThese replace the old tanh-normalized formula with the new additive formula.\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n# NEW REWARD FORMULA (Dec 2024)\n# ============================================================================\n\nimport numpy as np\n\n# New formula constants\nCOMPLIANCE_BASE = 0.80       # Base score at C=10 (log10(10)=1)\nCOMPLIANCE_SLOPE = 0.16      # Score decrease per log10 unit\nCOMPLIANCE_MIN = -0.50       # Minimum compliance score\nCOMPLIANCE_MAX = 0.85        # Maximum compliance score\n\nVOLUME_REFERENCE = 0.10      # Reference volume (V=0.10 gives no bonus/penalty)\nVOLUME_SENSITIVITY = 2.0     # Bonus/penalty per 0.1 volume change\nVOLUME_BONUS_MAX = 0.30      # Max bonus for very low V\nVOLUME_PENALTY_MAX = 0.60    # Max penalty for very high V\n\n\ndef calculate_compliance_score(compliance: float) -> float:\n    \"\"\"\n    Calculate the compliance component of the reward using log10 mapping.\n    \n    Mapping:\n    - C=10 (log10=1) -> 0.80\n    - C=1,000,000 (log10=6) -> 0.00\n    \"\"\"\n    if compliance <= 0:\n        return COMPLIANCE_MAX\n    \n    log_c = np.log10(max(compliance, 1.0))\n    score = COMPLIANCE_BASE - COMPLIANCE_SLOPE * (log_c - 1.0)\n    \n    return float(np.clip(score, COMPLIANCE_MIN, COMPLIANCE_MAX))\n\n\ndef calculate_volume_bonus(vol_frac: float) -> float:\n    \"\"\"\n    Calculate the volume bonus/penalty.\n    \n    Volume is ALWAYS inversely proportional to reward:\n    - V < VOLUME_REFERENCE -> positive bonus (lean structure)\n    - V > VOLUME_REFERENCE -> negative penalty (heavy structure)\n    \"\"\"\n    adjustment = (VOLUME_REFERENCE - vol_frac) * VOLUME_SENSITIVITY\n    return float(np.clip(adjustment, -VOLUME_PENALTY_MAX, VOLUME_BONUS_MAX))\n\n\ndef calculate_new_reward(compliance: float, vol_frac: float, is_valid: bool = True) -> float:\n    \"\"\"\n    Calculate the reward using the NEW additive formula.\n    \n    R = compliance_score(C) + volume_bonus(V)\n    \n    Returns:\n        Reward in range [-1, 1]\n    \"\"\"\n    if not is_valid:\n        return -1.0\n    \n    compliance_score = calculate_compliance_score(compliance)\n    volume_bonus = calculate_volume_bonus(vol_frac)\n    \n    reward = compliance_score + volume_bonus\n    \n    return float(np.clip(reward, -1.0, 1.0))\n\n\n# Test the new formula\nprint(\"üéØ NEW REWARD FORMULA TEST (at V=0.10):\")\nfor c in [10, 100, 1000, 10000, 100000, 1000000]:\n    r = calculate_new_reward(c, 0.10)\n    print(f\"   C={c:>8}: R={r:.3f}\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Configure Training\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\n# Training configuration\nCONFIG = {\n    'use_swin': False,          # False = SimpleBackbone, True = Swin-UNETR\n    'feature_size': 24,\n    'batch_size': 32,           # Per GPU (64 total with 2 GPUs)\n    'epochs': 30,\n    'learning_rate': 1e-4,\n    'weight_decay': 1e-5,\n    'num_workers': 0,\n    'use_amp': True,            # Mixed precision\n    'val_split': 0.1,\n    'patience': 10,             # Early stopping\n    'save_every': 5,            # Save checkpoint every N epochs\n    'recalculate_values': True, # NEW: Recalculate value targets with new formula\n}\n\nprint(f\"\\n‚öôÔ∏è Training Configuration:\")\nfor k, v in CONFIG.items():\n    print(f\"   {k}: {v}\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Import and Setup\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\nimport time\nimport json\nimport sqlite3\nfrom pathlib import Path\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split, Dataset\nfrom torch.amp import GradScaler, autocast\nfrom tqdm.auto import tqdm\n\nfrom alphabuilder.src.neural.model import AlphaBuilderV31\nfrom alphabuilder.src.neural.dataset import TopologyDatasetV31\nfrom alphabuilder.src.neural.trainer import policy_loss, weighted_value_loss, LAMBDA_POLICY\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## NEW: Custom Dataset with Value Recalculation\n\nWraps the base dataset but recalculates value targets using the new formula.\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n# WRAPPER DATASET THAT RECALCULATES VALUES\n# ============================================================================\n\nclass RewardRecalculationDataset(Dataset):\n    \"\"\"\n    Wraps TopologyDatasetV31 and recalculates value targets using new formula.\n    \n    This requires the database to have compliance and volume_fraction in metadata.\n    If not available, falls back to the original value.\n    \"\"\"\n    \n    def __init__(self, base_dataset, db_path: Path, recalculate: bool = True):\n        self.base_dataset = base_dataset\n        self.db_path = db_path\n        self.recalculate = recalculate\n        \n        # Build a mapping from index to compliance/volume\n        self.value_overrides = {}\n        \n        if recalculate:\n            self._load_compliance_volume_data()\n    \n    def _load_compliance_volume_data(self):\n        \"\"\"Load compliance and volume data from database metadata.\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        # Try v2 schema first (episodes + records)\n        try:\n            cursor.execute(\"\"\"\n                SELECT r.episode_id, r.step, e.metadata_blob\n                FROM records r\n                JOIN episodes e ON r.episode_id = e.episode_id\n            \"\"\")\n            rows = cursor.fetchall()\n            \n            for ep_id, step, meta_blob in rows:\n                if meta_blob:\n                    try:\n                        import zlib\n                        import pickle\n                        meta = pickle.loads(zlib.decompress(meta_blob))\n                        compliance = meta.get('compliance')\n                        vol_frac = meta.get('volume_fraction')\n                        \n                        if compliance is not None and vol_frac is not None:\n                            key = (ep_id, step)\n                            new_value = calculate_new_reward(compliance, vol_frac)\n                            self.value_overrides[key] = new_value\n                    except:\n                        pass\n        except sqlite3.OperationalError:\n            pass\n        \n        # Try v1 schema (training_data)\n        try:\n            cursor.execute(\"SELECT id, metadata FROM training_data\")\n            rows = cursor.fetchall()\n            \n            for rec_id, meta_json in rows:\n                if meta_json:\n                    try:\n                        meta = json.loads(meta_json)\n                        compliance = meta.get('compliance')\n                        vol_frac = meta.get('volume_fraction')\n                        \n                        if compliance is not None and vol_frac is not None:\n                            self.value_overrides[rec_id] = calculate_new_reward(compliance, vol_frac)\n                    except:\n                        pass\n        except sqlite3.OperationalError:\n            pass\n        \n        conn.close()\n        \n        print(f\"   üìä Loaded {len(self.value_overrides)} value overrides\")\n    \n    def __len__(self):\n        return len(self.base_dataset)\n    \n    def __getitem__(self, idx):\n        sample = self.base_dataset[idx]\n        \n        # Try to override value if we have recalculated data\n        if self.recalculate and len(self.value_overrides) > 0:\n            info = self.base_dataset.index[idx]\n            \n            # Try different key formats depending on schema\n            key = None\n            if 'episode_id' in info and 'step' in info:\n                key = (info['episode_id'], info['step'])\n            elif 'record_id' in info:\n                key = info['record_id']\n            \n            if key and key in self.value_overrides:\n                sample['value'] = torch.tensor([self.value_overrides[key]], dtype=torch.float32)\n        \n        return sample\n\nprint(\"‚úÖ RewardRecalculationDataset defined\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Create DataLoaders\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\nprint(f\"\\nüìÇ Loading dataset...\")\n\n# Load base dataset\nbase_dataset = TopologyDatasetV31(\n    db_path=Path(DATA_PATH),\n    augment=True,\n    preload_to_ram=True  # Kaggle has enough RAM\n)\n\nprint(f\"   Base samples: {len(base_dataset):,}\")\n\n# Wrap with value recalculation if enabled\nif CONFIG['recalculate_values']:\n    print(\"   üîÑ Wrapping dataset with value recalculation...\")\n    full_dataset = RewardRecalculationDataset(\n        base_dataset, \n        Path(DATA_PATH),\n        recalculate=True\n    )\nelse:\n    full_dataset = base_dataset\n\nprint(f\"   Total samples: {len(full_dataset):,}\")\n\n# Split train/val\nval_size = int(len(full_dataset) * CONFIG['val_split'])\ntrain_size = len(full_dataset) - val_size\n\ntrain_dataset, val_dataset = random_split(\n    full_dataset,\n    [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\nprint(f\"   Train samples: {len(train_dataset):,}\")\nprint(f\"   Val samples: {len(val_dataset):,}\")\n\nprint(f\"   DataLoaders will be created per-epoch for memory management\")",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Create Model\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\nprint(f\"\\nüß† Creating model...\")\n\ndevice = torch.device('cuda')\n\nmodel = AlphaBuilderV31(\n    in_channels=7,\n    out_channels=2,\n    feature_size=CONFIG['feature_size'],\n    use_swin=CONFIG['use_swin']\n)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"   Architecture: {'Swin-UNETR' if CONFIG['use_swin'] else 'SimpleBackbone'}\")\nprint(f\"   Total parameters: {total_params:,}\")\n\n# Multi-GPU\nif torch.cuda.device_count() > 1:\n    print(f\"   Using DataParallel on {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)\n\n# Optimizer\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay']\n)\n\n# Scheduler\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=CONFIG['epochs'],\n    eta_min=CONFIG['learning_rate'] * 0.01\n)\n\n# Mixed precision\nscaler = GradScaler('cuda') if CONFIG['use_amp'] else None\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Training Functions\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\ndef train_epoch(model, loader, optimizer, scaler, device):\n    model.train()\n    total_loss = 0\n    total_p_loss = 0\n    total_v_loss = 0\n    n_batches = 0\n    \n    pbar = tqdm(loader, desc=\"Training\", leave=False)\n    \n    for batch in pbar:\n        state = batch['state'].to(device, non_blocking=True)\n        target_policy = batch['policy'].to(device, non_blocking=True)\n        target_value = batch['value'].to(device, non_blocking=True)\n        \n        optimizer.zero_grad(set_to_none=True)\n        \n        if scaler is not None:\n            with autocast('cuda'):\n                pred_policy, pred_value = model(state)\n                v_loss = weighted_value_loss(pred_value, target_value)\n                p_loss = policy_loss(pred_policy, target_policy)\n                loss = v_loss + LAMBDA_POLICY * p_loss\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            pred_policy, pred_value = model(state)\n            v_loss = weighted_value_loss(pred_value, target_value)\n            p_loss = policy_loss(pred_policy, target_policy)\n            loss = v_loss + LAMBDA_POLICY * p_loss\n            \n            loss.backward()\n            optimizer.step()\n        \n        total_loss += loss.item()\n        total_p_loss += p_loss.item()\n        total_v_loss += v_loss.item()\n        n_batches += 1\n        \n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    return {\n        'loss': total_loss / n_batches,\n        'policy_loss': total_p_loss / n_batches,\n        'value_loss': total_v_loss / n_batches,\n    }\n\n\ndef validate_epoch(model, loader, device):\n    model.eval()\n    total_loss = 0\n    total_p_loss = 0\n    total_v_loss = 0\n    n_batches = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Validating\", leave=False):\n            state = batch['state'].to(device, non_blocking=True)\n            target_policy = batch['policy'].to(device, non_blocking=True)\n            target_value = batch['value'].to(device, non_blocking=True)\n            \n            with autocast('cuda'):\n                pred_policy, pred_value = model(state)\n                v_loss = weighted_value_loss(pred_value, target_value)\n                p_loss = policy_loss(pred_policy, target_policy)\n                loss = v_loss + LAMBDA_POLICY * p_loss\n            \n            total_loss += loss.item()\n            total_p_loss += p_loss.item()\n            total_v_loss += v_loss.item()\n            n_batches += 1\n    \n    return {\n        'loss': total_loss / n_batches,\n        'policy_loss': total_p_loss / n_batches,\n        'value_loss': total_v_loss / n_batches,\n    }\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Training Loop\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def get_memory_info():\n    \"\"\"Get current GPU and RAM memory usage.\"\"\"\n    import psutil\n    ram_percent = psutil.virtual_memory().percent\n    ram_used = psutil.virtual_memory().used / 1024**3\n    \n    if torch.cuda.is_available():\n        gpu_allocated = torch.cuda.memory_allocated() / 1024**3\n        gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n        return f\"RAM: {ram_used:.1f}GB ({ram_percent:.0f}%) | GPU: {gpu_allocated:.1f}GB alloc / {gpu_reserved:.1f}GB reserved\"\n    return f\"RAM: {ram_used:.1f}GB ({ram_percent:.0f}%)\"\n\nprint(f\"\\nüöÄ Starting training: {CONFIG['epochs']} epochs\")\nprint(\"-\" * 60)\nprint(f\"üìä Initial memory: {get_memory_info()}\")\n\ncheckpoint_dir = Path(\"/kaggle/working/checkpoints\")\ncheckpoint_dir.mkdir(exist_ok=True)\n\nbest_val_loss = float('inf')\npatience_counter = 0\nhistory = {'train_loss': [], 'val_loss': [], 'lr': []}\nbatch_size = CONFIG['batch_size'] * max(1, torch.cuda.device_count())\nnum_workers = CONFIG['num_workers']\n\ntraining_start = time.time()\n\nfor epoch in range(CONFIG['epochs']):\n    epoch_start = time.time()\n    \n    # Create fresh train loader\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=True,\n    )\n    \n    train_metrics = train_epoch(model, train_loader, optimizer, scaler, device)\n    \n    # Cleanup\n    del train_loader\n    import gc\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(f\"  üìä After train cleanup: {get_memory_info()}\")\n    \n    # Create fresh val loader\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    \n    val_metrics = validate_epoch(model, val_loader, device)\n    \n    del val_loader\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(f\"  üìä After val cleanup: {get_memory_info()}\")\n    \n    scheduler.step()\n    current_lr = scheduler.get_last_lr()[0]\n    \n    history['train_loss'].append(train_metrics['loss'])\n    history['val_loss'].append(val_metrics['loss'])\n    history['lr'].append(current_lr)\n    \n    epoch_time = time.time() - epoch_start\n    samples_per_sec = len(train_dataset) / epoch_time\n    \n    print(f\"\\nEpoch {epoch + 1}/{CONFIG['epochs']} ({epoch_time:.1f}s, {samples_per_sec:.0f} samples/s)\")\n    print(f\"  Train | Loss: {train_metrics['loss']:.4f} | P: {train_metrics['policy_loss']:.4f} | V: {train_metrics['value_loss']:.4f}\")\n    print(f\"  Val   | Loss: {val_metrics['loss']:.4f} | P: {val_metrics['policy_loss']:.4f} | V: {val_metrics['value_loss']:.4f}\")\n    print(f\"  LR: {current_lr:.2e}\")\n    \n    if val_metrics['loss'] < best_val_loss:\n        best_val_loss = val_metrics['loss']\n        patience_counter = 0\n        torch.cuda.empty_cache()\n        model_state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model_state,\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': best_val_loss,\n            'config': CONFIG,\n            'reward_formula': 'v3.2_additive',  # NEW: Track formula version\n        }, checkpoint_dir / \"best_model.pt\")\n        print(f\"  ‚úì New best model saved (val_loss: {best_val_loss:.4f})\")\n        del model_state\n    else:\n        patience_counter += 1\n    \n    if (epoch + 1) % CONFIG['save_every'] == 0:\n        model_state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model_state,\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_metrics['loss'],\n            'config': CONFIG,\n            'reward_formula': 'v3.2_additive',\n        }, checkpoint_dir / f\"checkpoint_epoch_{epoch + 1}.pt\")\n        del model_state\n    \n    if patience_counter >= CONFIG['patience']:\n        print(f\"\\n‚ö†Ô∏è Early stopping triggered (patience={CONFIG['patience']})\")\n        break",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Final Summary\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\ntotal_time = time.time() - training_start\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üéâ Training Complete!\")\nprint(\"=\" * 60)\nprint(f\"   Total time: {total_time / 3600:.1f} hours\")\nprint(f\"   Best validation loss: {best_val_loss:.4f}\")\nprint(f\"   Checkpoints saved to: {checkpoint_dir}\")\nprint(f\"   Reward formula: v3.2_additive\")\n\n# List saved files\nprint(f\"\\nüìÅ Saved files:\")\nfor f in sorted(checkpoint_dir.glob(\"*.pt\")):\n    print(f\"   {f.name} ({f.stat().st_size / 1024**2:.1f} MB)\")\n\n# Plot training history\ntry:\n    import matplotlib.pyplot as plt\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    \n    ax1.plot(history['train_loss'], label='Train')\n    ax1.plot(history['val_loss'], label='Val')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training Progress (v3.2 Additive Reward)')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    ax2.plot(history['lr'])\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Learning Rate')\n    ax2.set_title('LR Schedule')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(checkpoint_dir / \"training_history.png\", dpi=150)\n    plt.show()\n    print(f\"\\nüìä Training history saved to: {checkpoint_dir / 'training_history.png'}\")\nexcept Exception as e:\n    print(f\"\\n‚ö†Ô∏è Could not plot history: {e}\")\n\nprint(\"\\n‚úÖ Done! Download checkpoints from /kaggle/working/checkpoints/\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Listar arquivos\n!ls -la /kaggle/working/checkpoints/\n# Copiar para output root (mais vis√≠vel na UI)\n!cp /kaggle/working/checkpoints/*.pt /kaggle/working/\n!ls -la /kaggle/working/*.pt",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        }
    ]
}