{
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.13"
        },
        "accelerator": "GPU",
        "kaggle": {
            "accelerator": "nvidiaTeslaT4",
            "dataSources": [
                {
                    "sourceId": 14005693,
                    "sourceType": "datasetVersion",
                    "datasetId": 8923815
                }
            ],
            "dockerImageVersionId": 31193,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": true
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "AlphaBuilder v3.3 - Kaggle Training (Pre-Corrected Data)\n========================================================\n\nThis script is designed to run on Kaggle with GPU T4 x2.\n\n**VERSION 3.3 HIGHLIGHTS:**\n- Uses **pre-corrected database** (`warmup_data_v3_2.db`)\n- Value targets are already updated to the new additive reward formula\n- No runtime recalculation needed (faster data loading)\n\n**New Reward Formula (Reference):**\n- `reward = compliance_score(C) + volume_bonus(V)`\n\nExpected runtime: ~4.5 hours for 30 epochs\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Environment Setup\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\nimport subprocess\nimport sys\nimport os\n\nprint(\"=\"*60)\nprint(\"üöÄ AlphaBuilder v3.3 - Kaggle Training (Pre-Corrected Data)\")\nprint(\"=\"*60)\n\n# Check GPU\nimport torch\nprint(f\"\\nüìä Hardware Detection:\")\nprint(f\"   CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU count: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n        props = torch.cuda.get_device_properties(i)\n        print(f\"       Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Clone Repository\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\nREPO_URL = \"https://github.com/gustavomello9600/alphabuild.git\"\nREPO_DIR = \"/kaggle/working/alphabuild\"\n\nif not os.path.exists(REPO_DIR):\n    print(f\"\\nüì• Cloning repository...\")\n    subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\nelse:\n    print(f\"\\nüì• Updating repository...\")\n    subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\"], check=True)\n\n# Add to path\nsys.path.insert(0, REPO_DIR)\nos.chdir(REPO_DIR)\n\nprint(f\"   Working directory: {os.getcwd()}\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Data Setup\n\n**IMPORTANT**: Upload the corrected `warmup_data_v3_2.db` to Kaggle datasets and add it to this notebook.\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\n# Possible locations for the dataset\n# Priority: Kaggle Input -> Local Upload -> Fallback\nPOSSIBLE_PATHS = [\n    \"/kaggle/input/alphabuilder-warmup-v3-2/warmup_data_v3_2.db\",  \n    \"/kaggle/input/alphabuilder-warmup-data/warmup_data_v3_2.db\",   \n    \"/kaggle/working/warmup_data_v3_2.db\",                          \n]\n\nDATA_PATH = None\nfor path in POSSIBLE_PATHS:\n    if os.path.exists(path):\n        DATA_PATH = path\n        break\n\nif DATA_PATH is None:\n    print(\"\\n‚ö†Ô∏è Corrected dataset not found automatically.\")\n    print(\"   Looking for 'warmup_data_v3_2.db'...\")\n    # Fallback to standard name if user renamed it back\n    if os.path.exists(\"/kaggle/input/alphabuilder-warmup-data/warmup_data.db\"):\n        print(\"   Found 'warmup_data.db'. assuming this MIGHT be the new one if you renamed it.\")\n        DATA_PATH = \"/kaggle/input/alphabuilder-warmup-data/warmup_data.db\"\n    else:\n        raise FileNotFoundError(\"Could not find training database!\")\n\nprint(f\"\\nüìÇ Training data: {DATA_PATH}\")\nprint(f\"   Size: {os.path.getsize(DATA_PATH) / 1024**2:.1f} MB\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Configure Training\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\nCONFIG = {\n    'use_swin': False,          # False = SimpleBackbone, True = Swin-UNETR\n    'feature_size': 24,\n    'batch_size': 32,           # Per GPU\n    'epochs': 30,\n    'learning_rate': 1e-4,\n    'weight_decay': 1e-5,\n    'num_workers': 2,\n    'use_amp': True,\n    'val_split': 0.1,\n    'patience': 10,\n    'save_every': 5,\n    'recalculate_values': False, # DISABLED: Data is already correct\n}\n\nprint(f\"\\n‚öôÔ∏è Training Configuration:\")\nfor k, v in CONFIG.items():\n    print(f\"   {k}: {v}\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Import and Setup\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\nimport time\nimport json\nimport sqlite3\nfrom pathlib import Path\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.amp import GradScaler, autocast\nfrom tqdm.auto import tqdm\n\nfrom alphabuilder.src.neural.model import AlphaBuilderV31\nfrom alphabuilder.src.neural.dataset import TopologyDatasetV31\nfrom alphabuilder.src.neural.trainer import policy_loss, weighted_value_loss, LAMBDA_POLICY\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Load Data and Model\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n\nprint(f\"\\nüìÇ Loading dataset...\")\n\n# Direct loading of V3.3 dataset with corrected values\nfull_dataset = TopologyDatasetV31(\n    db_path=Path(DATA_PATH),\n    augment=True,\n    preload_to_ram=True\n)\n\nprint(f\"   Total samples: {len(full_dataset):,}\")\n\n# Split train/val\nval_size = int(len(full_dataset) * CONFIG['val_split'])\ntrain_size = len(full_dataset) - val_size\n\ntrain_dataset, val_dataset = random_split(\n    full_dataset,\n    [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\nprint(f\"   Train samples: {len(train_dataset):,}\")\nprint(f\"   Val samples: {len(val_dataset):,}\")\n\n# Model Setup\nprint(f\"\\nüß† Creating model based on {CONFIG['use_swin'] and 'Swin-UNETR' or 'SimpleBackbone'}...\")\ndevice = torch.device('cuda')\n\nmodel = AlphaBuilderV31(\n    in_channels=7,\n    out_channels=2,\n    feature_size=CONFIG['feature_size'],\n    use_swin=CONFIG['use_swin']\n)\n\nif torch.cuda.device_count() > 1:\n    print(f\"   Using DataParallel on {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay']\n)\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=CONFIG['epochs'],\n    eta_min=CONFIG['learning_rate'] * 0.01\n)\n\nscaler = GradScaler('cuda') if CONFIG['use_amp'] else None\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Training Loop\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ============================================================================\n# Standard training functions\n\ndef train_epoch(model, loader, optimizer, scaler, device):\n    model.train()\n    total_loss, total_p_loss, total_v_loss = 0, 0, 0\n    n_batches = 0\n    pbar = tqdm(loader, desc=\"Training\", leave=False)\n    \n    for batch in pbar:\n        state = batch['state'].to(device, non_blocking=True)\n        target_policy = batch['policy'].to(device, non_blocking=True)\n        target_value = batch['value'].to(device, non_blocking=True)\n        \n        optimizer.zero_grad(set_to_none=True)\n        \n        if scaler:\n            with autocast('cuda'):\n                pred_policy, pred_value = model(state)\n                v_loss = weighted_value_loss(pred_value, target_value)\n                p_loss = policy_loss(pred_policy, target_policy)\n                loss = v_loss + LAMBDA_POLICY * p_loss\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            pred_policy, pred_value = model(state)\n            v_loss = weighted_value_loss(pred_value, target_value)\n            p_loss = policy_loss(pred_policy, target_policy)\n            loss = v_loss + LAMBDA_POLICY * p_loss\n            loss.backward()\n            optimizer.step()\n        \n        total_loss += loss.item()\n        total_p_loss += p_loss.item()\n        total_v_loss += v_loss.item()\n        n_batches += 1\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    return {'loss': total_loss/n_batches, 'policy_loss': total_p_loss/n_batches, 'value_loss': total_v_loss/n_batches}\n\ndef validate_epoch(model, loader, device):\n    model.eval()\n    total_loss, total_p_loss, total_v_loss = 0, 0, 0\n    n_batches = 0\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Validating\", leave=False):\n            state = batch['state'].to(device, non_blocking=True)\n            target_policy = batch['policy'].to(device, non_blocking=True)\n            target_value = batch['value'].to(device, non_blocking=True)\n            \n            with autocast('cuda'):\n                pred_policy, pred_value = model(state)\n                v_loss = weighted_value_loss(pred_value, target_value)\n                p_loss = policy_loss(pred_policy, target_policy)\n                loss = v_loss + LAMBDA_POLICY * p_loss\n            \n            total_loss += loss.item()\n            total_p_loss += p_loss.item()\n            total_v_loss += v_loss.item()\n            n_batches += 1\n            \n    return {'loss': total_loss/n_batches, 'policy_loss': total_p_loss/n_batches, 'value_loss': total_v_loss/n_batches}\n\n# === Main Loop ===\n\nprint(f\"\\nüöÄ Starting training...\")\ncheckpoint_dir = Path(\"/kaggle/working/checkpoints\")\ncheckpoint_dir.mkdir(exist_ok=True)\n\nbest_val_loss = float('inf')\npatience_counter = 0\nhistory = {'train_loss': [], 'val_loss': [], 'lr': []}\nbatch_size = CONFIG['batch_size'] * max(1, torch.cuda.device_count())\n\ntraining_start = time.time()\n\nfor epoch in range(CONFIG['epochs']):\n    epoch_start = time.time()\n    \n    # === TRAIN ===\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n                              num_workers=CONFIG['num_workers'], pin_memory=True, drop_last=True)\n    train_metrics = train_epoch(model, train_loader, optimizer, scaler, device)\n    \n    # AGGRESSIVE CLEANUP to avoid OOM\n    del train_loader\n    import gc\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    # === VAL ===\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n                            num_workers=CONFIG['num_workers'], pin_memory=True)\n    val_metrics = validate_epoch(model, val_loader, device)\n    \n    # AGGRESSIVE CLEANUP\n    del val_loader\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    scheduler.step()\n    \n    history['train_loss'].append(train_metrics['loss'])\n    history['val_loss'].append(val_metrics['loss'])\n    history['lr'].append(scheduler.get_last_lr()[0])\n    \n    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']} | Train: {train_metrics['loss']:.4f} | Val: {val_metrics['loss']:.4f}\")\n    \n    # Save best\n    if val_metrics['loss'] < best_val_loss:\n        best_val_loss = val_metrics['loss']\n        patience_counter = 0\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': best_val_loss,\n            'config': CONFIG,\n            'reward_formula': 'v3.2_additive(pre_corrected)',\n        }, checkpoint_dir / \"best_model.pt\")\n        print(f\"  ‚úì Saved best model\")\n    else:\n        patience_counter += 1\n        \n    if patience_counter >= CONFIG['patience']:\n        print(\"‚ö†Ô∏è Early stopping\")\n        break\n\nprint(f\"\\n‚úÖ Done! Best Val Loss: {best_val_loss:.4f}\")\n",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Copy outputs\n!cp /kaggle/working/checkpoints/*.pt /kaggle/working/\n!ls -la /kaggle/working/*.pt",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        }
    ]
}