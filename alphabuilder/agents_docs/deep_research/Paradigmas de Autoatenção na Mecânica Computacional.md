# **Paradigmas de Autoatenção na Mecânica Computacional: Uma Análise Exaustiva da Transição de CNNs para Vision Transformers em Otimização Topológica e Predição de Campos de Tensão (2021-2025)**

## **1\. Introdução**

A disciplina de Mecânica Computacional, historicamente ancorada em métodos numéricos robustos como a Análise de Elementos Finitos (FEM), atravessa um período de transformação radical impulsionado pela integração com o Aprendizado Profundo (*Deep Learning*). Entre 2021 e 2025, a literatura científica registrou um cisma metodológico fundamental. A primeira onda de adoção de inteligência artificial na mecânica, predominante até meados de 2020, baseava-se extensivamente em Redes Neurais Convolucionais (CNNs), particularmente arquiteturas do tipo U-Net, para aproximar soluções de equações diferenciais parciais (PDEs). A premissa central era tratar domínios físicos discretizados — sejam campos de tensão, deformação ou distribuições de densidade material — como imagens digitais, aproveitando a eficácia das CNNs na extração de características visuais locais.  
No entanto, a complexidade inerente aos problemas de elasticidade linear e não-linear, especialmente aqueles que envolvem materiais heterogêneos e condições de contorno complexas, expôs as limitações intrínsecas das arquiteturas convolucionais. O viés indutivo de localidade das CNNs, embora vantajoso para a detecção de bordas, demonstrou-se insuficiente para capturar dependências globais de longo alcance (*long-range dependencies*), essenciais para a propagação de forças através de um meio contínuo. Em resposta a essas limitações, pesquisadores começaram a adaptar arquiteturas baseadas em mecanismos de Autoatenção (*Self-Attention*), originárias do Processamento de Linguagem Natural (NLP), especificamente os *Vision Transformers* (ViT).  
Este relatório apresenta uma análise profunda e sistemática da literatura produzida no período de 2021 a 2025, focando exclusivamente em abordagens "livres de iteração" (*iteration-free*). Estas abordagens buscam substituir os solvers iterativos tradicionais e os loops de otimização (como SIMP ou MMA) por inferências diretas de redes neurais. A análise disseca como os Transformers estão sendo aplicados para superar as CNNs em duas frentes críticas: a Otimização Topológica (TO) e a previsão de campos de tensão (surrogate modeling), oferecendo uma síntese das arquiteturas emergentes, como Swin Transformers, TransUNets e modelos híbridos, e suas respectivas vantagens na modelagem de fenômenos físicos complexos.

## **2\. Fundamentos Teóricos e Convergência Arquitetural**

Para compreender a transição das CNNs para os Transformers na mecânica, é imperativo analisar as bases teóricas que justificam a superioridade do mecanismo de atenção na resolução de problemas de valor de contorno.

### **2.1. O Limite do Campo Receptivo nas CNNs**

As redes neurais convolucionais operam através da aplicação de kernels locais (filtros) que deslizam sobre o domínio de entrada. Matematicamente, isso se assemelha a operadores diferenciais locais. Para que uma informação presente em um nó i da malha influencie a predição em um nó j distante, a rede deve ser suficientemente profunda para que os campos receptivos dos neurônios nas camadas subsequentes abranjam ambos os pontos. Em mecânica estrutural, isso é problemático. Pelo princípio de Saint-Venant, o efeito de uma carga aplicada em uma extremidade de uma estrutura é sentido globalmente. As CNNs, como a U-Net, tentam mitigar isso através de operações de *pooling* (downsampling), mas esse processo frequentemente resulta na perda de detalhes de alta frequência e em imprecisões nas fronteiras dos materiais ou nas regiões de concentração de tensão.

### **2.2. Mecanismo de Autoatenção como Operador Global**

O mecanismo de autoatenção (*Self-Attention*), central nos Transformers, rompe com a restrição de localidade. Ele permite que cada "token" (uma representação vetorial de um patch da malha ou imagem) atenda a todos os outros tokens da sequência simultaneamente, calculando uma soma ponderada das representações baseada em sua relevância mútua.  
No contexto da elasticidade linear, onde o deslocamento u é resolvido através da inversão da matriz de rigidez global K (i.e., u \= K^{-1}f), a matriz inversa K^{-1} é densa, implicando que uma força pontual afeta todo o campo de deslocamento. A autoatenção simula essa conectividade densa, permitindo que o modelo aprenda correlações globais complexas em uma única camada, sem a necessidade de profundidade excessiva ou downsampling agressivo. Estudos recentes indicam que o mecanismo de atenção melhora a capacidade de modelagem global superior às CNNs, integrando os pontos fortes de ambos ao capturar características sutis e globais.  
\#\#\# 2.3. Arquiteturas Emergentes na Mecânica (2021-2025)  
A literatura analisada revela a consolidação de três arquiteturas principais adaptadas para problemas físicos:

1. **Pure Vision Transformers (ViT):** Onde o domínio físico é dividido em *patches* não sobrepostos e processado inteiramente como uma sequência de vetores. Esta abordagem é favorecida para capturar a física global em otimização topológica.  
2. **Swin Transformers:** Uma variante hierárquica que utiliza janelas deslizantes (*shifted windows*) para calcular a atenção. Isso reintroduz a localidade e a eficiência computacional, sendo crucial para prever campos densos com alta resolução, como em problemas de torque em motores ou segmentação de materiais.  
3. **TransUNet / Swin-Unet:** Arquiteturas híbridas que inserem blocos de transformadores no *bottleneck* (gargalo) ou no codificador de uma estrutura U-Net. O objetivo é combinar a extração de características locais das convoluções com a modelagem de contexto global dos transformadores, essencial para capturar detalhes finos de fronteiras em otimização topológica multiobjetivo.

## **3\. Otimização Topológica "Iteration-Free" via Transformers**

A Otimização Topológica (TO) tradicionalmente depende de métodos iterativos baseados em gradiente, como o Método das Assíntotas Móveis (MMA) ou critérios de otimalidade, que requerem centenas de avaliações de elementos finitos. A fronteira da pesquisa atual busca substituir esse processo iterativo por uma inferência direta (*one-shot*).

### **3.1. Vision Transformers Puros para Geração de Topologia**

Um dos avanços mais significativos em 2025 é a aplicação de arquiteturas puras de Transformers para a geração direta de topologias ótimas. O trabalho de Lutheran et al. (2025) introduz uma metodologia onde as condições de entrada (cargas, condições de contorno) e o domínio de design são tratados como sequências de tokens.  
Ao contrário das abordagens baseadas em CNN, que frequentemente geram estruturas desconectadas ("ilhas" de material) ou fronteiras difusas devido à incapacidade de correlacionar regiões distantes do domínio, o modelo baseado em Transformer demonstra uma capacidade superior de manter a integridade estrutural global. A pesquisa indica que, para alavancar modelos de transformadores na otimização estrutural, é necessário "patchificar" a imagem de entrada, convertendo o domínio espacial em uma coleção de regiões menores e de tamanho fixo.  
**Vantagem Crítica:** A principal vantagem reportada é a capacidade de gerar estruturas de alta fidelidade que se aproximam daquelas produzidas por métodos iterativos de difusão (*diffusion models*), mas com uma eficiência computacional ordens de magnitude superior, pois elimina a necessidade de passos de *denoising* iterativos ou loops de solver FEM.

### **3.2. Swin Transformers em Otimização de Máquinas Elétricas**

A aplicação de Transformers não se restringe à mecânica estrutural pura, mas estende-se à otimização multifísica. Nagayama e Sasaki (2025) demonstraram a eficácia dos **Swin Transformers** na previsão de características de torque para a otimização topológica de motores de relutância síncronos.  
Neste estudo, o Swin Transformer foi comparado diretamente a uma CNN de referência. Os resultados foram contundentes: o modelo baseado em Transformer melhorou a precisão da previsão do torque médio em **56,8%** (Erro Quadrático Médio). A análise qualitativa sugeriu que o mecanismo de atenção permitiu ao modelo "compreender" as barreiras de fluxo magnético — características geométricas finas e globais que definem o desempenho do motor — de uma maneira que as convoluções locais falharam em capturar. Além disso, o uso de atenção proporcionou uma interpretabilidade superior, permitindo a visualização de quais regiões do rotor eram mais críticas para a geração de torque.

### **3.3. Abordagens Híbridas e Multiobjetivo**

A complexidade aumenta quando múltiplos objetivos (ex: rigidez e dissipação de calor) devem ser satisfeitos simultaneamente. Trabalhos recentes exploraram o uso de **SwinUnet** para otimização topológica multiobjetivo. A estrutura hierárquica do Swin Transformer, combinada com a arquitetura U-Net, permite uma reconstrução de sinal de alta fidelidade, superando tanto a TransUNet quanto a SwinUnet padrão em métricas de desempenho.  
A tabela a seguir resume as principais contribuições em Otimização Topológica baseada em Transformers:

| Trabalho | Arquitetura | Aplicação | Vantagem sobre CNN/U-Net | Referência |
| :---- | :---- | :---- | :---- | :---- |
| **Transformer-based Topology Optimization** | Pure ViT (Patch-based) | Geração de Estrutura Estática | Elimina iterações; preserva continuidade global melhor que U-Net; comparável a modelos de difusão em qualidade. |  |
| **Predicting torque characteristics...** | Swin Transformer | Otimização de Motor (Torque) | **56.8%** de redução de erro (MSE); melhor captura de fluxo magnético não-local. |  |
|  | **Two-stage MO-TO via SwinUnet** | SwinUnet | Otimização Multiobjetivo | Reconstrução de alta fidelidade de fronteiras complexas; robustez em esparsidade elevada. |
| **Real-time TO via Deep Learning** | Self-Attention Network | Otimização em Tempo Real | Eficiência computacional em cenários multiobjetivo sem iterações. |  |

## **4\. Modelagem Substituta (Surrogate Modeling) e Previsão de Campos de Tensão**

A segunda grande vertente de aplicação refere-se à criação de modelos substitutos (*surrogates*) capazes de prever campos físicos (tensão, deformação, deslocamento) instantaneamente, substituindo a análise de elementos finitos (FEA).

### **4.1. O Desafio dos Materiais Heterogêneos**

A previsão de resposta mecânica em materiais compósitos e heterogêneos é notoriamente difícil devido às interações complexas entre inclusões (fibras, partículas) e a matriz. Métodos espectrais como o Operador Neural de Fourier (FNO) têm dificuldades com descontinuidades de alta frequência (interfaces de materiais), enquanto as CNNs falham em capturar interações de longo alcance entre inclusões distantes.  
O modelo **Micrometer** (Micromechanics Transformer), proposto por Wang et al. (2024/2025), aborda exatamente essa lacuna. Utilizando uma arquitetura baseada em Vision Transformer, o Micrometer alcança um erro menor que **1%** na previsão de campos de tensão macroscópicos, com uma redução de tempo computacional de até duas ordens de magnitude em comparação com solvers numéricos tradicionais. O estudo destaca explicitamente que o Micrometer supera tanto a U-Net quanto o FNO em precisão, atribuindo esse sucesso à capacidade do mecanismo de atenção de modelar as interações globais críticas em microestruturas de compósitos reforçados por fibra.

### **4.2. Plasticidade e Dependência de Trajetória**

A maioria dos modelos de *deep learning* na mecânica foca em elasticidade linear. No entanto, o comportamento não-linear (plasticidade) introduz dependência de trajetória (*path-dependency*), onde o estado atual depende de todo o histórico de carregamento. Tradicionalmente, isso seria tratado por Redes Neurais Recorrentes (RNNs, LSTMs, GRUs).  
Zhou e Semnani (2025) propuseram o **ViT-Transformer**, um modelo inovador para modelagem constitutiva de materiais heterogêneos não-lineares. O modelo utiliza um codificador ViT para extrair características da microestrutura (imagens de RVE) e um decodificador Transformer (semelhante ao GPT) para processar a sequência de deformação macroscópica. **Inovação Técnica:** O estudo introduz um algoritmo de "Treinamento de Extração Aleatória" (*Random Extract Training*) para aumentar a robustez a sequências de comprimento variável. A comparação direta mostra que o decodificador baseado em autoatenção supera as arquiteturas baseadas em GRU ao processar sequências longas de deformação, mitigando o problema de "esquecimento" inerente às RNNs e capturando melhor as dependências temporais complexas da plasticidade.

### **4.3. Aprendizado de Operadores e Independência de Malha**

Uma limitação das CNNs e ViTs padrão é a dependência da resolução da malha de treinamento. O conceito de "Operador Neural" visa aprender o operador de solução da PDE, permitindo a avaliação em qualquer resolução. O **CViT** (Continuous Vision Transformer), introduzido por Wang et al. (2025), propõe uma arquitetura que combina um codificador ViT com um decodificador que utiliza *embeddings* de coordenadas baseados em grade. Isso permite que o modelo capture dependências espaciais multiescala e avalie a solução em coordenadas arbitrárias, superando arquiteturas estabelecidas como o DeepONet e o FNO em benchmarks de dinâmica de fluidos e mecânica de sólidos. A capacidade do CViT de generalizar através de geometrias variáveis e resoluções posiciona-o como um avanço significativo para a criação de "gêmeos digitais" robustos.

## **5\. Convergências Interdisciplinares: Geofísica e Engenharia Civil**

A análise da literatura revela que os avanços na aplicação de Transformers para previsão de campos físicos não estão isolados na mecânica computacional clássica, mas permeiam campos adjacentes como a geofísica (inversão sísmica) e a engenharia civil (monitoramento estrutural e de túneis). Estas aplicações servem como validações cruzadas da eficácia das arquiteturas de atenção para problemas de propagação de ondas e deformação estrutural.

### **5.1. Inversão Sísmica e Quantificação de Incerteza**

A inversão sísmica, análoga à tomografia de tensão, busca reconstruir propriedades do subsolo a partir de dados de ondas. O modelo **BPI-ViT** (Bayesian Physics-Informed Vision Transformer), proposto por Liu et al. (2025), representa o estado da arte nesta área. Este trabalho é crucial pois introduz a **quantificação de incerteza em nível estrutural**. Enquanto PINNs (Redes Neurais Informadas por Física) tradicionais focam na minimização de resíduos de PDE ponto a ponto, o BPI-ViT integra autoatenção multicamada com inferência Bayesiana. Isso permite não apenas prever a estrutura geológica, mas também visualizar a incerteza associada à geometria das falhas e horizontes, superando métodos anteriores em detecção de anomalias e continuidade espacial.

### **5.2. Gêmeos Digitais de Túneis (Hybrid Fusion)**

No contexto de engenharia de túneis, a previsão de campos de deslocamento (*full-field displacement*) induzidos por escavação é crítica para a segurança. Um estudo de 2025 propôs o **DF-STTN** (Dual-Fusion Swin-Table Transformer Network). **Diferencial Arquitetural:** Este modelo é híbrido, combinando um **Swin Transformer** para processar dados espaciais (geometria do túnel, estratigrafia) com um **TabTransformer** para processar dados tabulares não espaciais (parâmetros operacionais da máquina de escavação, leituras de sensores pontuais). A fusão destas modalidades através de atenção cruzada permitiu que o DF-STTN superasse modelos baseados em convolução na previsão de assentamento do solo e deformação do revestimento, demonstrando que a inclusão de informação espacial global via Transformers é decisiva para a precisão em problemas geotécnicos complexos.

## **6\. Análise Comparativa de Arquiteturas: ViT vs. CNN**

A transição tecnológica documentada neste relatório não é meramente uma troca de algoritmos, mas uma mudança na representação fundamental do problema físico. A tabela abaixo sintetiza a comparação direta entre as abordagens de Transformers e CNNs (U-Net) baseada nos resultados quantitativos e qualitativos extraídos dos artigos.

| Característica | Redes Neurais Convolucionais (U-Net/ResNet) | Vision Transformers (ViT/Swin/TransUNet) | Evidência na Literatura (2021-2025) |
| :---- | :---- | :---- | :---- |
| **Mecanismo de Extração** | Local (Kernels deslizantes). | Global (Autoatenção) e Hierárquico (Swin). | ViTs capturam melhor fluxos magnéticos e caminhos de carga longos. |
| **Dependências de Longo Alcance** | Limitadas pela profundidade da rede e downsampling. Perda de sinal em longas distâncias. | Nativas. Cada token pode acessar informações de qualquer outro token instantaneamente. | Superioridade em materiais heterogêneos (Micrometer) e plasticidade dependente de história. |
| **Preservação de Detalhes** | Tende a suavizar bordas e "borrar" interfaces de materiais (difusão espectral). | TransUNet e Swin preservam detalhes de alta frequência e bordas nítidas. | Melhor segmentação de falhas sísmicas e fronteiras de topologia. |
| **Custo Computacional** | Eficiente em hardware, mas requer muitas camadas para contexto global. | Custo quadrático (ViT puro) mitigado por janelas (Swin). Inferência *one-shot* mais rápida que solvers iterativos. | Micrometer é 100x mais rápido que FEM ; TO via ViT é mais rápido que Difusão. |
| **Generalização** | Dificuldade com malhas irregulares ou variação de escala. | CViT e Neural Operators permitem independência de resolução (Mesh-agnostic). | CViT supera DeepONet em generalização multiescala. |

### **6.1. O Papel das Arquiteturas Híbridas**

É notável que, embora os Transformers puros (ViT) sejam poderosos, a literatura converge para soluções híbridas para tarefas de previsão de campo denso. Arquiteturas como **TransUNet** e **Swin-Unet** dominam os benchmarks porque utilizam CNNs para extrair características locais de baixo nível (texturas, pequenas inclusões) e Transformers para modelar o contexto global (forma estrutural, distribuição de carga). Estudos mostram que o TransUNet alcança precisão de segmentação (IoU) de até 94,24% em tarefas de manufatura aditiva, validando essa sinergia.

## **7\. Desafios de Implementação e Estratégias de Treinamento**

A adoção de Transformers na mecânica não é isenta de desafios. A literatura identifica obstáculos específicos e as soluções técnicas emergentes.

### **7.1. Custo de Treinamento e Dados**

Transformers carecem do viés indutivo de localidade das CNNs, o que significa que eles geralmente requerem conjuntos de dados maiores para aprender a estrutura espacial dos problemas físicos.

* **Estratégia:** O uso de **Data Augmentation** física (rotação, espelhamento, deformação elástica de input) é onipresente. O "Random Extract Training" proposto por Zhou e Semnani é um exemplo de inovação algorítmica para maximizar a utilidade de dados limitados de sequências de deformação.  
* **Transfer Learning:** O pré-treinamento em grandes datasets de imagens naturais (ex: ImageNet) seguido de *fine-tuning* em dados físicos é uma estratégia validada. O uso de modelos pré-treinados (como o ViT-B/16) permite convergência mais rápida mesmo com datasets de física limitados.

### **7.2. Tratamento de Entradas Não-Visuais**

Diferente de imagens RGB, os inputs de mecânica (forças, condições de contorno) são esparsos e vetoriais.

* **Estratégia:** A "tokenização" criativa é essencial. Lutheran et al. demonstram como incorporar condições de contorno como tokens especiais ou canais adicionais na representação de patches, permitindo que o Transformer processe a física juntamente com a geometria. A fusão de dados tabulares via TabTransformer (no DF-STTN) aponta para um futuro onde dados de sensores e simulações são fundidos nativamente.

## **8\. Conclusão**

A revisão exaustiva da literatura científica do período 2021-2025 confirma que a aplicação de **Vision Transformers (ViT)** e mecanismos de **Autoatenção** na mecânica computacional ultrapassou a fase de experimentação especulativa para se tornar uma alternativa robusta e, em muitos casos, superior às Redes Neurais Convolucionais (CNNs).  
A vantagem decisiva dos Transformers reside na sua capacidade topológica de simular a conectividade global de sistemas físicos (análoga à matriz de rigidez inversa), superando o viés local das convoluções. Isso se traduz em ganhos quantificáveis: redução de erros de previsão em mais de 50% em aplicações eletromecânicas , aceleração de duas ordens de magnitude em homogeneização micromecânica e a capacidade de gerar topologias ótimas complexas sem iterações.  
Para a comunidade de engenharia e pesquisa, a implicação é clara: para o desenvolvimento de modelos substitutos de alta fidelidade e sistemas de otimização em tempo real, as arquiteturas baseadas em **Swin Transformer** e híbridos **ViT-CNN** representam o novo padrão-ouro. A convergência entre a teoria de operadores neurais (CViT) e a arquitetura de transformadores sugere um futuro onde a distinção entre "aprender a geometria" e "aprender a física" se tornará cada vez mais difusa, permitindo a criação de Gêmeos Digitais verdadeiramente preditivos e agnósticos à discretização.

#### **Referências citadas**

1\. A Convolutional Neural Network-Based Stress Prediction Method for Airfoil Structures, https://www.mdpi.com/2226-4310/11/12/1057 2\. Real-time multi-objective topology optimization method via deep learning \- ResearchGate, https://www.researchgate.net/publication/384002308\_Real-time\_multi-objective\_topology\_optimization\_method\_via\_deep\_learning 3\. Convolutional Neural Network–Vision Transformer Architecture with Gated Control Mechanism and Multi-Scale Fusion for Enhanced Pulmonary Disease Classification \- MDPI, https://www.mdpi.com/2075-4418/14/24/2790 4\. An Improved Swin Transformer-Based Model for Remote Sensing Object Detection and Instance Segmentation \- MDPI, https://www.mdpi.com/2072-4292/13/23/4779 5\. Transformer-based Topology Optimization \- arXiv, https://arxiv.org/html/2509.05800v2 6\. Predicting torque characteristics of synchronous reluctance motors ..., https://www.emerald.com/compel/article/44/5/814/1297709/Predicting-torque-characteristics-of-synchronous 7\. (PDF) Two stage multiobjective topology optimization method via SwinUnet with enhanced generalization \- ResearchGate, https://www.researchgate.net/publication/389945483\_Two\_stage\_multiobjective\_topology\_optimization\_method\_via\_SwinUnet\_with\_enhanced\_generalization 8\. Transformers and large language models in healthcare: A review \- PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11638972/ 9\. \[2509.05800\] Transformer-based Topology Optimization \- arXiv, https://arxiv.org/abs/2509.05800 10\. Micrometer: Micromechanics Transformer for Predicting Mechanical Responses of Heterogeneous Materials \- ResearchGate, https://www.researchgate.net/publication/384242002\_Micrometer\_Micromechanics\_Transformer\_for\_Predicting\_Mechanical\_Responses\_of\_Heterogeneous\_Materials 11\. (PDF) Micrometer: Micromechanics transformer for predicting full field mechanical responses of heterogeneous materials \- ResearchGate, https://www.researchgate.net/publication/396445936\_Micrometer\_Micromechanics\_transformer\_for\_predicting\_full\_field\_mechanical\_responses\_of\_heterogeneous\_materials 12\. Micrometer: Micromechanics Transformer for Predicting Mechanical Responses of Heterogeneous Materials \- arXiv, https://arxiv.org/html/2410.05281v1 13\. ViT-Transformer: Self-attention mechanism based constitutive modeling for nonlinear heterogeneous materials \- arXiv, https://arxiv.org/html/2510.16575v1 14\. (PDF) ViT-Transformer: Self-attention mechanism based constitutive modeling for nonlinear heterogeneous materials \- ResearchGate, https://www.researchgate.net/publication/396714904\_ViT-Transformer\_Self-attention\_mechanism\_based\_constitutive\_modeling\_for\_nonlinear\_heterogeneous\_materials 15\. CViT: Continuous Vision Transformer for Operator Learning \- arXiv, https://arxiv.org/html/2405.13998v3 16\. (PDF) CViT: Continuous Vision Transformer for Operator Learning \- ResearchGate, https://www.researchgate.net/publication/388954002\_CViT\_Continuous\_Vision\_Transformer\_for\_Operator\_Learning 17\. (PDF) Review of Physics-Informed Machine Learning Inversion of Geophysical Data, https://www.researchgate.net/publication/383051412\_Review\_of\_Physics-Informed\_Machine\_Learning\_Inversion\_of\_Geophysical\_Data 18\. (PDF) Physics-aware Bayesian Vision Transformer enables structure-level uncertainty in seismic inversion \- ResearchGate, https://www.researchgate.net/publication/393699598\_Physics-aware\_Bayesian\_Vision\_Transformer\_enables\_structure-level\_uncertainty\_in\_seismic\_inversion 19\. A transformer-based surrogate modeling strategy for tunnel digital ..., https://www.researchgate.net/publication/397651254\_A\_transformer-based\_surrogate\_modeling\_strategy\_for\_tunnel\_digital\_twin\_in\_full-field\_displacement\_prediction\_under\_adjacent\_tunnel\_construction 20\. Application Research on Contour Feature Extraction of Solidified Region Image in Laser Powder Bed Fusion Based on SA-TransUNet \- MDPI, https://www.mdpi.com/2076-3417/15/5/2602 21\. Beyond Imaging: Vision Transformer Digital Twin Surrogates for 3D+T Biological Tissue Dynamics \- arXiv, https://arxiv.org/html/2508.15883v1