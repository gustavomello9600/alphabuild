(self, in_channels: 'int', out_channels: 'int', patch_size: 'int' = 2, depths: 'Sequence[int]' = (2, 2, 2, 2), num_heads: 'Sequence[int]' = (3, 6, 12, 24), window_size: 'Sequence[int] | int' = 7, qkv_bias: 'bool' = True, mlp_ratio: 'float' = 4.0, feature_size: 'int' = 24, norm_name: 'tuple | str' = 'instance', drop_rate: 'float' = 0.0, attn_drop_rate: 'float' = 0.0, dropout_path_rate: 'float' = 0.0, normalize: 'bool' = True, norm_layer: 'type[LayerNorm]' = <class 'torch.nn.modules.normalization.LayerNorm'>, patch_norm: 'bool' = False, use_checkpoint: 'bool' = False, spatial_dims: 'int' = 3, downsample: 'str | nn.Module' = 'merging', use_v2: 'bool' = False) -> 'None'