# AlphaBuilder v3.1: Especificação Técnica Mestra

**Projeto:** AlphaBuilder - Generative Topology Optimization

**Versão:** 3.1.0 (Dynamic Augmentation & Hard Negatives)

**Status:** REVISADO PARA IMPLEMENTAÇÃO

**Responsável Técnico:** Arquitetura de IA

**Target Hardware:** NVIDIA A100/H100 (Treino), T4/L4 (Inferência)

---

## 0. Introdução ao AlphaBuilder

Para contextualizar novos integrantes: O **AlphaBuilder** é um agente de Inteligência Artificial autônomo projetado para resolver problemas complexos de Engenharia Estrutural (Otimização Topológica). Ao contrário de softwares tradicionais que apenas analisam uma peça desenhada por humanos, o AlphaBuilder **projeta** a peça do zero.

Ele atua como um engenheiro especialista digital: recebe um "volume vazio" com forças e suportes definidos, e decide voxel a voxel onde colocar material para criar a estrutura mais leve e resistente possível. O sistema combina uma **Rede Neural** (que possui "intuição" física aprendida de milhares de simulações) com um algoritmo de busca **MCTS** (que explora estratégias de construção passo a passo), permitindo criar soluções inovadoras que respeitam rigorosamente as leis da física.

---

## 1. Visão Executiva e Mudanças de Paradigma

O AlphaBuilder v3.1 marca uma transição fundamental de um modelo puramente imitativo para um agente generativo híbrido. O sistema foi redesenhado para possuir **visão de longo prazo** (via Value Oracle) e **liberdade criativa** (via Unconstrained MCTS).

### Principais Diferenciais da Arquitetura v3.1:

1.  **Independência de Resolução:** A rede neural agora é agnóstica às dimensões espaciais do problema, utilizando *Dynamic Padding* para processar grids de qualquer tamanho.

2.  **Física de Alta Fidelidade:** O tensor de entrada foi expandido para **7 canais**, permitindo a distinção precisa entre diferentes tipos de condições de contorno (engastes, roletes, trilhos) e vetores de força.

3.  **Geração Paramétrica (Bézier):** Substituição de algoritmos de busca (A*) por geração procedural baseada em Curvas de Bézier com seções retangulares variáveis, criando *priors* estruturais organicamente eficientes.

4.  **Value Head Híbrida:** A estimativa de valor opera em dois regimes temporais distintos — previsão de potencial futuro na fase de construção e avaliação de qualidade física imediata na fase de refinamento.

5.  **MCTS Sem Travas:** O algoritmo de busca não possui mais restrições rígidas de fase ("apenas adicionar" ou "apenas remover"). O agente é livre para corrigir erros de construção ou reforçar estruturas fracas a qualquer momento.

---

## 2. Pipeline de Engenharia de Dados (The Data Factory)

O sucesso do modelo depende da riqueza e da física correta dos dados de treinamento. O pipeline foi reestruturado para ensinar não apenas "o que é uma estrutura ótima", mas "como chegar lá". **Nota:** Nesta versão, o banco de dados armazena apenas estados fisicamente coerentes; os exemplos negativos são gerados dinamicamente (ver Seção 4).

### 2.1. O Gerador Paramétrico (Ground Truth Fase 1)

O objetivo é gerar um volume grande de estruturas iniciais viáveis que sirvam como "Blueprint" (Planta Baixa) para o agente.

*   **Configuração do Cenário:**

    *   Grid base variável (padrão de desenvolvimento: $64 \times 32 \times 8$).

    *   **Condições de Contorno:** Face $X=0$ definida como suporte (variando entre engaste total ou parcial para enriquecer o dataset).

    *   **Cargas:** Carga vertical distribuída num plano de tensão 2x2 (para evitar singularidades numéricas) posicionada aleatoriamente em $X \ge 50\%$.

*   **Algoritmo de Geração (Bézier 3D):**

    1.  Sortear $N \in [1, 3]$ curvas de Bézier Quadráticas conectando a região de suporte à região de carga.

    2.  Aplicar ruído gaussiano aos pontos de controle intermediários para garantir diversidade topológica (curvas para cima, para baixo, direita e esquerda em S).

    3.  **Voxelização Volumétrica:** Ao longo de cada curva, extrudar uma seção transversal retangular $W(t) \times H(t)$ que varia linearmente.

        *   *Base:* Mais robusta.

        *   *Ponta:* Mais fina.

    4.  O resultado é a máscara binária `V_constructed`.

### 2.2. O Refinador Físico (Ground Truth Fase 2)

Utiliza-se o solver SIMP (validado via *fenitop*) para otimizar a `V_constructed`.

*   **Comportamento:** O SIMP deve ser configurado para reduzir o volume inicial até 10% do domínio.

*   **Observação Crítica:** O SIMP moderno pode **adicionar** material para reconectar ilhas ou reforçar áreas de tensão. O pipeline de extração de dados deve capturar essas adições, não apenas remoções.

*   **Score Final ($S_{final}$):** Ao final da otimização, calcula-se a Compliance e o Volume finais para gerar uma pontuação normalizada que servirá de "Oráculo".

### 2.3. Estratégia de "Slicing" e Targets

Cada episódio de geração resulta em múltiplos registros de treinamento no banco de dados, categorizados por fase.

#### Tipo A: Fase 1 (Construção / Blueprint)

Ensina a rede a visualizar a estrutura completa a partir do vazio ou de estágios parciais.

*   **Amostragem:** Gerar **50 fatias** temporais da construção do `V_constructed` baseadas na distância geodésica a partir do suporte (0% a 100% construído).

*   **Input ($X$):** O grid parcialmente preenchido.

*   **Target Policy ($Y_{pol}$):**

    *   *Canal Add:* A máscara completa de `V_constructed` menos o input atual. (Ensina: "Falta tudo isso aqui").

    *   *Canal Remove:* Zeros (Ignorado pela Loss).

*   **Target Value ($Y_{val}$):** **$S_{final}$ (Oracle Value).**

    *   *Lógica:* "Embora este estado esteja incompleto (fisicamente inválido), ele pertence a uma trajetória que resultará em uma estrutura de Score $S_{final}$".

#### Tipo B: Fase 2 (Refinamento / Otimização)

Ensina a rede a limpar o excesso de material e corrigir falhas locais.

*   **Amostragem:** Frames do histórico de convergência do SIMP.

*   **Input ($X$):** Densidade atual do frame $t$.

*   **Target Policy ($Y_{pol}$):** Diferença entre Frame $t$ e Frame $t+k$.

    *   *Canal Add:* Onde a densidade aumentou (reparo/conexão).

    *   *Canal Remove:* Onde a densidade diminuiu (otimização).

*   **Target Value ($Y_{val}$):** **$S_{current}$ (Physical Reality).**

    *   *Lógica:* Na Fase 2, o Value deve refletir a qualidade física *atual* do estado (calculada via FEM naquele frame).

---

## 3. Arquitetura Neural (The Brain)

O modelo deve ser capaz de generalizar para qualquer resolução e compreender a física vetorial.

### 3.1. Especificação do Tensor de Entrada

O input foi expandido para capturar a complexidade total das condições de contorno.

**Rank:** 5D `(Batch, 7, D, H, W)`

**Canais:**

1.  **Densidade ($\rho$):** Estado do material ($0.0$ a $1.0$).

2.  **Mask X:** $1.0$ se o deslocamento $u_x$ for travado, $0.0$ caso contrário.

3.  **Mask Y:** $1.0$ se o deslocamento $u_y$ for travado.

4.  **Mask Z:** $1.0$ se o deslocamento $u_z$ for travado.

5.  **Força X ($F_x$):** Componente normalizada da força.

6.  **Força Y ($F_y$):** Componente normalizada da força.

7.  **Força Z ($F_z$):** Componente normalizada da força.

### 3.2. Modelo: Swin-UNETR Agnóstico

*   **Backbone:** Swin Transformer 3D.

*   **Mecanismo de Entrada Dinâmica:**

    *   A rede não deve ter input shape fixo no código (ex: `img_size=(64,32,32)`).

    *   No método `forward(x)`, implementar **Dynamic Padding**:

        1.  Ler dimensões atuais $(D, H, W)$.

        2.  Calcular padding necessário para que cada dimensão seja divisível pelo tamanho da janela do Swin (geralmente 32 ou 64).

        3.  Aplicar `F.pad`.

        4.  Processar na rede.

        5.  Aplicar *Crop* na saída para retornar às dimensões originais $(D, H, W)$.

*   **Normalização:** **`InstanceNorm3d`**.

    *   *Justificativa:* Como processamos volumes 3D pesados, o batch size será minúsculo (2 ou 4). Batch Normalization falha estatisticamente com batches pequenos. Instance Norm é robusta e independente do batch.

### 3.3. Cabeças de Saída (Heads)

*   **Policy Head:**

    *   Saída: `(Batch, 2, D, H, W)`.

    *   Canal 0: Logits de Adição ($L_{add}$).

    *   Canal 1: Logits de Remoção ($L_{rem}$).

*   **Value Head:**

    *   Saída: `(Batch, 1)`.

    *   Ativação Final: `Tanh` (Força intervalo $[-1, 1]$).

---

## 4. Protocolo de Treinamento

### 4.1. Data Augmentation Dinâmico e Negative Sampling

Para evitar a "Alucinação de Estabilidade" e o armazenamento excessivo de dados, as transformações e a geração de exemplos negativos ocorrem *on-the-fly* no `DataLoader`, sem persistência no banco de dados.

**Fluxo de Augmentation por Ponto de Dado:**

1.  **Augmentation Física (Rígida):**

    *   Rotação 90º (eixo Z) e Flip (com inversão correta dos vetores de Força $F$).

2.  **Negative Sampling & Stress Test (Probabilístico):**

    O sistema decide aplicar **UM** dos seguintes efeitos destrutivos baseando-se nas propriedades do ponto de dado atual:

    *   **Condition A: Final Step (Prioridade Máxima)**

        *   *Gatilho:* Se o ponto de dado é o estado final de um episódio (estrutura ótima).

        *   *Ação:* **Erosion Attack (100% de chance).** Aplica-se erosão morfológica 3D.

        *   *Target Value:* **-1.0**.

        *   *Target Policy:* Diferença entre original e erodido (ensina a restaurar espessura).

    *   **Condition B: Connected State**

        *   *Gatilho:* Se a estrutura conecta todas as cargas aos suportes (verificado via Connected Components), mas não é o passo final.

        *   *Chance:* **5%** de aplicar **Load Multiplier**.

        *   *Ação:* Multiplicar as Forças ($F$) no tensor de entrada por um fator $K=3.0$.

        *   *Target Value:* **-0.8** (Simulando colapso por tensão).

        *   *Target Policy:* Adição nas zonas de tensão estimada.

    *   **Condition C: General Sabotage**

        *   *Chance:* **5%** de aplicar **Sabotagem Sistemática**.

        *   *Ação:* Remover voxels especificamente em nós de conexão ou adjacentes aos engastes.

        *   *Target Value:* **-1.0**.

        *   *Target Policy:* Adição imediata dos voxels removidos.

    *   **Condition D: The Saboteur**

        *   *Chance:* **10%** de aplicar **Sabotador Padrão**.

        *   *Ação:* Remover um cubo aleatório de voxels que desconecte o caminho de carga.

        *   *Target Value:* **-1.0**.

        *   *Target Policy:* Adição (Reparo).

### 4.2. Normalização de Valor (Value Scaling)

Compliance ($C$) varia exponencialmente (de $10^1$ a $10^5$). Treinar diretamente com $C$ causa explosão de gradiente.

**Fórmula de Mapeamento Log-Squash:**

$$ S_{raw} = -\log(C + \epsilon) - \alpha \cdot Vol $$

$$ Target_{Value} = \tanh\left( \frac{S_{raw} - \mu_{dataset}}{\sigma_{dataset}} \right) $$

*   Isso mapeia estruturas ruins para $\approx -1$ e estruturas ótimas para $\approx 1$.

### 4.3. Função de Perda Ponderada (Weighted Loss)

A Loss deve respeitar a intenção de cada fase e penalizar drasticamente o falso otimismo em estados negativos gerados pelo augmentation.

$$ L_{total} = L_{value} + \lambda \cdot L_{policy} $$

*   **Weighted Value Loss:**

    Para combater o desbalanceamento onde a maioria dos dados são "bons", aplicamos um peso extra ($w_{neg}$) quando o target é negativo.

    $$ L_{value} = \begin{cases} (y_{pred} - y_{gt})^2 & \text{se } y_{gt} > 0 \\ \mathbf{w_{neg}} \cdot (y_{pred} - y_{gt})^2 & \text{se } y_{gt} \le 0 \end{cases} $$

    *   *Sugestão:* $w_{neg} = 5.0$.

*   **Policy Loss:**

    *   *Fase 1 (Blueprint):* Foco no Canal Add. Mascarar Canal Remove.

    *   *Fase 2 (Refino) & Negatives:* Foco em ambos canais. Aplicar `pos_weight` para lidar com a esparsidade das ações de correção.

---

## 5. Engenharia de Inferência (MCTS)

O motor de inferência MCTS deve ser otimizado para latência e throughput, utilizando o modelo neural como guia.

### 5.1. Dinâmica Unconstrained (Sem Travas)

O MCTS recebe os logits completos da rede ($L_{add}, L_{rem}$) e os converte em probabilidades.

*   **Não existem regras rígidas** do tipo "Se Fase 1, proibir remoção".

*   O MCTS avalia ações de Adição e Remoção em pé de igualdade.

*   A "fase" é um fenômeno emergente:

    *   No início, o $L_{add}$ será alto (Blueprint), então o MCTS naturalmente escolhe adicionar.

    *   Se o MCTS cometer um erro, o $L_{rem}$ ou a queda no Value farão ele corrigir (remover) o erro.

### 5.2. Otimização de Performance (Micro-Batches & Batch Inference)

Para viabilizar o tempo de resposta:

1.  **Micro-Batches de Ação:** O MCTS não executa 1 voxel por vez. Ele seleciona os **Top-8 voxels** mais promissores da Policy e os aplica em conjunto no ambiente de simulação.

2.  **Inferência Paralela (Batch Inference):**

    *   O MCTS deve rodar múltiplas simulações (threads/workers) em paralelo.

    *   As requisições de avaliação neural (`network.predict(state)`) dessas threads devem ser acumuladas em uma fila.

    *   Um despachante envia um **Batch único** (ex: 16 ou 32 estados) para a GPU, maximizando o uso do hardware.

### 5.3. Função de Recompensa e Constraint

*   **Fase 1 (Desconectado):**

    *   Penalidade de vida (`-0.01/step`) para incentivar rapidez.

    *   Bônus de Conexão ao atingir o suporte.

*   **Fase 2 (Conectado):**

    *   Recompensa = $\Delta Volume \times \text{QualidadeFísica}$.

    *   **Hard Constraint:** A cada passo (batch de 8), verifica-se o Deslocamento Máximo ($U_{max}$) via solver rápido ou estimativa.

    *   Se $U_{max} > Limit$: Recompensa = **-1.0**. O estado é marcado como terminal/falha.

